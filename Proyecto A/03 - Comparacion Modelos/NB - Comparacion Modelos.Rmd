---
title: "Comparacion de Modelos"
output: html_document
---

# Paquetes

```{r}
library(caret)
library(readr)
library(dplyr)
library(ggplot2)
library(doMC) 
```

# Dataset

```{r}
# Linux
data <- read_csv('Proyecto A/00 - CreaciÃ³n Dataset y Variables/Dataset_Modelos.csv')
```

Eliminamos las variables con caracteres para crear las particiones de train y test.

```{r}
char_vars <- c("X1", 
               "url", 
               "scheme", 
               "domain_complete", 
               "domain", 
               "subdomain", 
               "suffix", 
               "domain_subdomain", 
               "path", 
               "suffix2")

df <- data %>%
  select(-char_vars)
```

```{r}
set.seed(45)
train_index <- createDataPartition(y = df$phishing, times = 1, p = 0.80, list = FALSE)

train <- df[train_index,]
test <- df[-train_index,]

train <- train %>%
  mutate(phishing = factor(ifelse(phishing==1, "Phish", "NoPhish")))
test <- test %>%
  mutate(phishing = factor(ifelse(phishing==1, "Phish", "NoPhish")))

#train <- train[,1:44]
#test <- test[,1:44]

rm(df)
```



```{r}
getwd()
gbm <- readRDS("Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/GBM.rds")
rf <- readRDS("Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/RF.rds")
ridge <- readRDS("Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/Ridge.rds")
lasso <- readRDS("Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/Lasso.rds")
```

# Train

```{r}
mat_ridge1 <- confusionMatrix(data = as.factor(predict(ridge, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
print(mat_ridge1)
```


```{r}
mat_lasso1 <- confusionMatrix(data = as.factor(predict(lasso, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
print(mat_lasso1)
```

```{r}
mat_rf1 <- confusionMatrix(data = as.factor(predict(rf, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_gbm1 <- confusionMatrix(data = as.factor(predict(gbm, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
```

# Test

```{r}
mat_ridge2 <- confusionMatrix(data = as.factor(predict(ridge, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_lasso2 <- confusionMatrix(data = as.factor(predict(lasso, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_rf2 <- confusionMatrix(data = as.factor(predict(rf, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_gbm2 <- confusionMatrix(data = as.factor(predict(gbm, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```



```{r}
twoClassSummary()
```



```{r}
models <- c("Ridge", "Lasso", "RF", "GBM")
results <- list(mat_ridge1, mat_lasso1, mat_rf1, mat_gbm1)
metrics <- c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall")

vec_models <- c()
vec_data <- c()
vec_metric <- c()
vec_value <- c()

i = 1
for(result in results){
  for(metric in metrics){
    vec_models <- append(vec_models, models[i])
    vec_data <- append(vec_data, "Train")
    vec_metric <- append(vec_metric, metric)
    if(metric == "Accuracy"){
      vec_value <- append(vec_value, result$overall[metric])  
    } else {
      vec_value <- append(vec_value, result$byClass[metric])  
    }
  }
  i = i + 1
}

train_results <- data.frame("Models" = vec_models,
                            "Data" = vec_data,
                            "Metric" = vec_metric,
                            "Value" = vec_value)

```

```{r}
models <- c("Ridge", "Lasso", "RF", "GBM")
results <- list(mat_ridge2, mat_lasso2, mat_rf2, mat_gbm2)
metrics <- c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall")

vec_models <- c()
vec_data <- c()
vec_metric <- c()
vec_value <- c()

i = 1
for(result in results){
  for(metric in metrics){
    vec_models <- append(vec_models, models[i])
    vec_data <- append(vec_data, "Test")
    vec_metric <- append(vec_metric, metric)
    if(metric == "Accuracy"){
      vec_value <- append(vec_value, result$overall[metric])  
    } else {
      vec_value <- append(vec_value, result$byClass[metric])  
    }
  }
  i = i + 1
}

test_results <- data.frame("Models" = vec_models,
                            "Data" = vec_data,
                            "Metric" = vec_metric,
                            "Value" = vec_value)

```

```{r}
results <- rbind(train_results, test_results)
write_csv(results, file = "Proyecto A/03 - Comparacion Modelos/Resultados.csv")
```


```{r}
results %>%
  filter(Metric == "Accuracy") %>%
  ggplot(aes(x = reorder(as.factor(Models), -Value), y = Value, color = Data)) +
  geom_point(size = 3) +
  theme_bw()
```


