{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Paquetes usados:\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# inicio y fin indican la cantidad de páginas de Phish Search a scrapear.\n",
    "inicio = 1\n",
    "fin = 1\n",
    "urls = [0] * (fin - inicio + 1)\n",
    "\n",
    "# Se crea una lista \"urls\" con las páginas a scrapear.\n",
    "for i, page in enumerate(range(inicio,fin + 1)):\n",
    "    url = 'https://www.phishtank.com/phish_search.php?page='+str(page)+'&valid=n&Search=Search'\n",
    "    urls[i] = url\n",
    "\n",
    "print('Listado de URLs a scrapear')\n",
    "print(urls)\n",
    "\n",
    "# Este diccionario tendrá los dataframes generados en cada URL scrapeado.\n",
    "dict_df = {}\n",
    "\n",
    "\n",
    "# Para cada url en la lista de urls se genera un Selector List con la información del html.\n",
    "for n, url in enumerate(urls):\n",
    "    # Obtenemos el html de la url con un request.\n",
    "    html = requests.get(url).content\n",
    "    # Creamos el Selector de Scrapy para acceder a los datos del html.\n",
    "    selector = Selector(text = html)\n",
    "    # path directamente va a la parte de las tablas del html, donde está el listado de links.\n",
    "    path = '//table[@class=\"data\"]/tr'\n",
    "    # Creamos la lista de selectores con los elementos de la tabla.\n",
    "    # Tenemos 21 elementos, 20 links y uno en blanco que es el primero.\n",
    "    selector_list = selector.xpath(path)\n",
    "    print(len(selector_list))\n",
    "\n",
    "    df = pd.DataFrame(np.zeros((20,6)), columns=['ID', 'URL_esp', 'URL', 'Completo', 'Valid', 'Online'])\n",
    "\n",
    "    '''\n",
    "    Se omite la posición 0 por no tener información.\n",
    "    Para cada elemento del selector se obtiene la siguiente información.\n",
    "    * td[1] tiene la información del id del link de phishing analizado. \n",
    "      También se puede obtener el url específico de dicho link (sirve para links que no aparecen completos.)\n",
    "    * td[2] tiene el link sospechoso de phishing (si termina en '...' está incompleto)\n",
    "    * td[4] tiene el valor si es 'VALID' o 'INVALID'. Deberían ser todos 'INVALID' que son los confirmados NO phishing.\n",
    "    * td[5] tiene el valor si está 'online' y 'offline'.\n",
    "    '''\n",
    "    for i in range(1,len(selector_list)):\n",
    "        # Analizamos la información de un elemento particular.\n",
    "        # El td[1] tiene info del id.\n",
    "        iden = selector_list[i].xpath('./td[1]/a/text()').extract()[0]\n",
    "        # Si necesitamos el url específico.\n",
    "        url_esp = selector_list[i].xpath('./td[1]/a/@href').extract()[0]\n",
    "        # Obtengo el link del sitio en cuestión.\n",
    "        url = selector_list[i].xpath('./td[2]/text()').extract()[0]\n",
    "        completo = int(1)\n",
    "        # En caso de que el link obtenido termine en '...' debemos acceder al url_esp (o usando el id) para obtenerlo completo.\n",
    "        if url[-3:] == '...':\n",
    "            # Hacemos el request usando el id.\n",
    "            #html_particular = requests.get('https://www.phishtank.com/phish_detail.php?phish_id='+str(iden)).content\n",
    "\n",
    "            url = \"REQUEST\"\n",
    "            completo = int(0)     \n",
    "        \n",
    "        valid = selector_list[i].xpath('./td[4]/strong/text()').extract()[0]\n",
    "        online = selector_list[i].xpath('./td[5]/text()').extract()[0]\n",
    "\n",
    "        df.iloc[i-1] = [iden, url_esp, url, completo, valid, online]\n",
    "\n",
    "    dict_df['df_'+str(n)] = df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict_df['df_0']"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}